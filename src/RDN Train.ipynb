{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uV8Lch-wnW32"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fltk46feoNyW"},"outputs":[],"source":["# cd drive/My \\Drive/....."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"both","id":"agXdpFwPPiHw"},"outputs":[],"source":["'''\n","Pranath Reddy\n","Benchmark Notebook for Superresolution\n","Model: Residual Dense Network (RDN)\n","'''\n","\n","# Import required libraries\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from tqdm.notebook import tqdm\n","from torch import autograd\n","from torchvision import models\n","import torch.utils.model_zoo as model_zoo\n","import math\n","from skimage.metrics import structural_similarity as ssim\n","from sklearn.utils import shuffle\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","# Load training data\n","# High-Resolution lensing data\n","x_trainHR = np.load('./Data/train_HR.npy').astype(np.float32).reshape(-1,1,150,150) \n","# Low-Resolution lensing data\n","x_trainLR = np.load('./Data/train_LR.npy').astype(np.float32).reshape(-1,1,75,75)\n","x_trainHR = torch.Tensor(x_trainHR)\n","x_trainLR = torch.Tensor(x_trainLR)\n","# Print data dimensions\n","print(x_trainHR.shape)\n","print(x_trainLR.shape)\n","\n","# Create dataset and dataloader for efficient data loading and batching\n","dataset = TensorDataset(x_trainLR, x_trainHR)\n","dataloader = DataLoader(dataset, batch_size=8)\n","\n","# Define DenseLayer class, which represents a single dense layer\n","class DenseLayer(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DenseLayer, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=3 // 2)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        return torch.cat([x, self.relu(self.conv(x))], 1)\n","\n","# Define RDB (Residual Dense Block) class\n","class RDB(nn.Module):\n","    def __init__(self, in_channels, growth_rate, num_layers):\n","        super(RDB, self).__init__()\n","        self.layers = nn.Sequential(*[DenseLayer(in_channels + growth_rate * i, growth_rate) for i in range(num_layers)])\n","\n","        # local feature fusion\n","        self.lff = nn.Conv2d(in_channels + growth_rate * num_layers, growth_rate, kernel_size=1)\n","\n","    def forward(self, x):\n","        # local residual learning\n","        return x + self.lff(self.layers(x))  \n","\n","# Define RDN (Residual Dense Network) class\n","class RDN(nn.Module):\n","    def __init__(self, scale_factor=2, num_channels=1, num_features=32, growth_rate=32, num_blocks=6, num_layers=4):\n","        super(RDN, self).__init__()\n","        self.G0 = num_features\n","        self.G = growth_rate\n","        self.D = num_blocks\n","        self.C = num_layers\n","\n","        # shallow feature extraction\n","        self.sfe1 = nn.Conv2d(num_channels, num_features, kernel_size=3, padding=3 // 2)\n","        self.sfe2 = nn.Conv2d(num_features, num_features, kernel_size=3, padding=3 // 2)\n","\n","        # residual dense blocks\n","        self.rdbs = nn.ModuleList([RDB(self.G0, self.G, self.C)])\n","        for _ in range(self.D - 1):\n","            self.rdbs.append(RDB(self.G, self.G, self.C))\n","\n","        # global feature fusion\n","        self.gff = nn.Sequential(\n","            nn.Conv2d(self.G * self.D, self.G0, kernel_size=1),\n","            nn.Conv2d(self.G0, self.G0, kernel_size=3, padding=3 // 2)\n","        )\n","\n","        # up-sampling\n","        assert 2 <= scale_factor <= 4\n","        if scale_factor == 2 or scale_factor == 4:\n","            self.upscale = []\n","            for _ in range(scale_factor // 2):\n","                self.upscale.extend([nn.Conv2d(self.G0, self.G0 * (2 ** 2), kernel_size=3, padding=3 // 2),\n","                                     nn.PixelShuffle(2)])\n","            self.upscale = nn.Sequential(*self.upscale)\n","        else:\n","            self.upscale = nn.Sequential(\n","                nn.Conv2d(self.G0, self.G0 * (scale_factor ** 2), kernel_size=3, padding=3 // 2),\n","                nn.PixelShuffle(scale_factor)\n","            )\n","\n","        self.output = nn.Conv2d(self.G0, num_channels, kernel_size=3, padding=3 // 2)\n","\n","    def forward(self, x):\n","        sfe1 = self.sfe1(x)\n","        sfe2 = self.sfe2(sfe1)\n","\n","        x = sfe2\n","        local_features = []\n","        for i in range(self.D):\n","            x = self.rdbs[i](x)\n","            local_features.append(x)\n","\n","        # global residual learning\n","        x = self.gff(torch.cat(local_features, 1)) + sfe1  \n","        x = self.upscale(x)\n","        x = self.output(x)\n","        return x\n","\n","# Set the device to use for training\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Pass the model to the device\n","model = RDN().to(device)\n","\n","# Set the loss criterion and optimizer\n","criteria = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n","\n","# Set the number of training epochs and learning rate scheduler\n","n_epochs = 100\n","scheduler = optim.lr_scheduler.OneCycleLR(optimizer, 2e-4, epochs=n_epochs, steps_per_epoch=x_trainHR.shape[0])\n","\n","# Training loop\n","loss_array = []\n","for epoch in tqdm(range(1, n_epochs+1)):\n","    train_loss = 0.0\n","    \n","    for data in dataloader:\n","\n","        # Fetch HR, LR data and pass to device\n","        datalr = data[0]\n","        datahr = data[1]\n","        datalr = datalr.to(device)\n","        datahr = datahr.to(device)\n","\n","        # Forward pass: compute predicted outputs by passing inputs to the model\n","        outputs = model(datalr)\n","        # Calculate the loss\n","        loss = criteria(outputs, datahr)\n","\n","        # Reset the gradients\n","        optimizer.zero_grad()\n","        # Perform a backward pass (backpropagation)\n","        loss.backward()\n","        # Update the parameters\n","        optimizer.step()\n","        # Update the learning rate\n","        scheduler.step()\n","\n","         # Update the training loss\n","        train_loss += (loss.item()*datahr.size(0))\n","        \n","    # Print average training statistics\n","    train_loss = train_loss/x_trainHR.shape[0]\n","    loss_array.append(train_loss)\n","\n","    # Save model and training loss\n","    torch.save(model, './Weights/RDN.pth')\n","    np.save('Results/RDN_Loss.npy', loss_array)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}

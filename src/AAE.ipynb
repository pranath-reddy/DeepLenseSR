{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uV8Lch-wnW32"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fltk46feoNyW"},"outputs":[],"source":["# cd drive/My \\Drive/....."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"both","id":"agXdpFwPPiHw"},"outputs":[],"source":["import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from tqdm.notebook import tqdm\n","from torch import autograd\n","\n","x_train = np.load('./Data/train_HR.npy')\n","x_train = x_train.astype(np.float32)\n","print(x_train.shape)\n","\n","class Encoder(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 16, 7, stride=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, 7, stride=3, padding=1)\n","        self.conv3 = nn.Conv2d(32, 64, 7)\n","        self.flat = nn.Flatten()\n","        self.linear = nn.Linear(5184, 1000)\n","\n","    def forward(self, x):\n","        \n","        convolution1 = F.relu(self.conv1(x))\n","        convolution2 = F.relu(self.conv2(convolution1))\n","        convolution3 = F.relu(self.conv3(convolution2))\n","        Flattened = self.flat(convolution3)\n","        z = self.linear(Flattened)\n","\n","        return z\n","        \n","class Decoder(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.linear = nn.Linear(1000, 5184)\n","        self.conv4 = nn.ConvTranspose2d(64, 32, 7)\n","        self.conv5 = nn.ConvTranspose2d(32, 16, 7, stride=3, padding=1, output_padding=2)\n","        self.conv6 = nn.ConvTranspose2d(16, 1, 6, stride=3, padding=1, output_padding=2)\n","\n","    def forward(self, x):\n","\n","        hidden = self.linear(x)\n","        Reshaped = hidden.reshape(-1,64,9,9)\n","        convolution4 = F.relu(self.conv4(Reshaped))\n","        convolution5 = F.relu(self.conv5(convolution4))\n","        predicted = torch.tanh(self.conv6(convolution5))\n","\n","        return predicted\n","\n","class Discriminator(nn.Module):\n","\n","    def __init__(self, dim_z=1000 , dim_h=256):\n","        super(Discriminator,self).__init__()\n","        self.dim_z = dim_z\n","        self.dim_h = dim_h\n","        self.network = []\n","        self.network.extend([\n","            nn.Linear(self.dim_z, self.dim_h),\n","            nn.ReLU(),\n","            nn.Linear(self.dim_h, self.dim_h),\n","            nn.ReLU(),\n","            nn.Linear(self.dim_h,1),\n","            nn.Sigmoid(),\n","        ])\n","        self.network = nn.Sequential(*self.network)\n","\n","    def forward(self, z):\n","        disc = self.network(z)\n","        return disc\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","encoder = Encoder().to(device)\n","decoder = Decoder().to(device)\n","Disc = Discriminator().to(device)\n","\n","optim_encoder = torch.optim.Adam(encoder.parameters(), lr=0.001)\n","optim_decoder = torch.optim.Adam(decoder.parameters(), lr=0.001)\n","optim_D = torch.optim.Adam(Disc.parameters(), lr=0.001)\n","optim_encoder_reg = torch.optim.Adam(encoder.parameters(), lr=0.0001)\n","\n","EPS = 1e-15\n","ae_criterion = nn.MSELoss()\n","n_epochs = 300\n","loss_array = []\n","pbar = tqdm(range(1, n_epochs+1))\n","for epoch in pbar:\n","    total_rec_loss = 0\n","    total_disc_loss = 0\n","    total_gen_loss = 0\n","    \n","    for i in range(x_train.shape[0]):\n","\n","        data = torch.from_numpy(x_train[i])\n","        if torch.cuda.is_available():\n","          data = data.cuda()\n","\n","        ### Encoder/Decoder\n","        encoding = encoder(data)\n","        fake = decoder(encoding)\n","        ae_loss = ae_criterion(fake, data)\n","        total_rec_loss += ae_loss.item()*data.size(0)\n","        \n","        optim_encoder.zero_grad()\n","        optim_decoder.zero_grad()\n","        ae_loss.backward()\n","        optim_encoder.step()\n","        optim_decoder.step()\n","\n","        ### Discriminator\n","        z_real_gauss = autograd.Variable(torch.randn(100, 1000) * 5.).to(device)\n","        D_real_gauss = Disc(z_real_gauss)\n","\n","        z_fake_gauss = encoder(data)\n","        D_fake_gauss = Disc(z_fake_gauss)\n","\n","        D_loss = -torch.mean(torch.log(D_real_gauss + EPS) + torch.log(1 - D_fake_gauss + EPS))\n","        total_disc_loss += D_loss.item()*data.size(0)\n","\n","        optim_D.zero_grad()\n","        D_loss.backward()\n","        optim_D.step()\n","\n","        ### Generator\n","        z_fake_gauss = encoder(data)\n","        D_fake_gauss = Disc(z_fake_gauss)\n","\n","        G_loss = -torch.mean(torch.log(D_fake_gauss + EPS))\n","        total_gen_loss += G_loss.item()*data.size(0)\n","\n","        optim_encoder_reg.zero_grad()\n","        G_loss.backward()\n","        optim_encoder_reg.step()\n","\n","    train_loss = total_rec_loss/x_train.shape[0]\n","    loss_array.append(train_loss)\n","\n","    torch.save(encoder, './Weights/AAE/AAE_Enc.pth')\n","    torch.save(decoder, './Weights/AAE/AAE_Dec.pth')\n","\n","    pbar.set_postfix({ 'Recon Loss': train_loss })\n","    np.save('Results/AAE_loss.npy', loss_array)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}
